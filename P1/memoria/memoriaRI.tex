\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla, es-lcroman, es-noquoting]{babel}
\usepackage[left=3cm,top=3.5cm,right=3cm,bottom=3.5cm]{geometry} 
\selectlanguage{spanish}
\usepackage{graphicx} %package to manage images
\graphicspath{ {./fotos/} }
\usepackage{enumerate} % enumerados
\usepackage{color}
\usepackage{listings}
\usepackage{makeidx}
\usepackage{hyperref}
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{
\normalfont \normalsize
\horrule{0.5pt} \\[0.4cm]
\huge Práctica 1: \\ Parser de documentos con TIKA. \\
\horrule{2pt} \\[0.5cm]
\begin{figure}[h]
\centering
\includegraphics[scale=0.75]{logoUGR.jpg}
\textsc{\textbf{Recuperación de Información (2020-2021)}} \\ [10pt]
\end{figure}\
}

\author{ Daniel Bolaños Martínez \\ Fernando de la Hoz Moreno \\ Grupo 10 - Martes 11:30h}
\date{}

\lstset{ %
language=Java,                % choose the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=none,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)}          % if you want to add a comment within your code
}

\begin{document}

\maketitle

\newpage

\tableofcontents

\clearpage

\section{Introducción.}

En esta práctica veremos cómo poder extraer información (texto) a partir de un documento dado, independiente del formato del archivo, paso previo para cualquier proceso de recuperación de información o análisis de textos.\cite{Guion}\\

Se pide realizar un programa capaz de extraer toda la información que cuelga de un directorio que contenga como mínimo 10 ficheros distintos con al menos 3 formatos diferentes y que según la opción, realice las siguientes funciones:

\begin{itemize}
\item \textbf{-d}: Creación de una tabla con nombre, codificación, tipo e idioma de cada fichero.
\item \textbf{-l}: Extracción de todos los enlaces de cada fichero.
\item \textbf{-t}: Generación de un archivo CSV que contenga las ocurrencias de las palabras para cada fichero.
\end{itemize}

Antes de comenzar a analizar y parsear la lista de archivos del directorio debemos, para todas las opciones:

\begin{itemize}
\item Obtener la lista con los nombres de todos los ficheros contenidos en el directorio pasado por parámetro.
\item Definir un objeto \textit{tika} y establecer el límite de caracteres.
\end{itemize}

\begin{lstlisting}
File directorio = new File(args[1]);
String[] archivos = directorio.list();
//Creamos un objeto Tika
Tika tika = new Tika();
//Establecemos el limite de caracteres para los strings
tika.setMaxStringLength(1000000000);
//Representa los metadatos del documento
Metadata metadata = new Metadata();
\end{lstlisting}\

Los documentos que queramos analizar los debemos almacenar en el directorio \textit{docs} dentro de la carpeta \textit{src}.

\section{Opción \textbf{-d}.}

Para esta opción, crearemos una tabla que recopile para cada archivo del directorio, información sobre su nombre, codificación, tipo de documento y lenguaje.\\

Declaramos un manejador de tipo \textit{BodyContentHandler} estableciendo el límite de caracteres y definimos los objetos para el contexto y parser (usamos \textit{AutoParser} para crear un \textit{parser} específico según el tipo del fichero). Estos objetos, serán pasados como parámetros a la función \textit{parse} junto con el \textit{FileInputStream} del fichero que estamos analizando.\\ 

A partir de los metadatos de cada archivo, podemos extraer el nombre del fichero (RESOURCE\_NAME\_KEY), la codificación (CONTENT\_ENCODING) y el tipo (CONTENT\_TYPE).\cite{Guion}\\

Finalmente, para identificar el lenguaje declaramos un objeto \textit{LanguageIdentifier} y le pasamos un string con el contenido en texto plano del archivo.\\ 

\textbf{Nota}: Usamos la función \textit{tika.parse()} para completar algunos metadatos que aparecen como \textit{null} usando el método \textit{parse}.\\

\begin{lstlisting}
//Recorremos para cada archivo del directorio
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Establece el limite de caracteres en el constructor
  BodyContentHandler handler = new BodyContentHandler(1000000000);
  //Guarda la informacion del contexto concreto para el ContentHandler
  ParseContext parseContext = new ParseContext();
  //Creamos un objeto para parsear el documento
  AutoDetectParser parser = new AutoDetectParser();
  /*Utilizamos parser para que a traves del stream de datos, content, nos devuelva el contenido en el handler y los metadatos en medatada*/
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, handler, metadata, parseContext);
  } finally{
      stream.close();}
  //Usamos tika.parse() para obtener metadatos mas completos.
  tika.parse(f, metadata);
  /*Creamos un objeto LanguageIdentifier al que le pasamos un string con el contenido del documento para extraer el idioma en el que este escrito.*/
  LanguageIdentifier object = new LanguageIdentifier(handler.toString());
  /*Extraemos el nombre del documento, la codificacion, el MIME tipo del archivo y el idioma en que esta escrito.*/
  String nam = metadata.get(Metadata.RESOURCE_NAME_KEY);
  String encod = metadata.get(Metadata.CONTENT_ENCODING);
  String type = metadata.get(Metadata.CONTENT_TYPE);
  String lang = object.getLanguage();
}
\end{lstlisting}\

Para mostrar por pantalla la tabla, hemos creado un formato con un tabulado personalizado que nos proporciona los resutados de la Figura \ref{tabla}.

\section{Opción \textbf{-l}.}

Para esta opción, queremos extraer todos los enlaces para cada fichero del directorio pasado como parámetro. Para ello iteramos sobre cada fichero del directorio.\\

Declaramos un manejador de tipo \textit{LinkContentHandler} que como hemos visto en los ejemplos, es necesario para obtener los links. \cite{tika} Tenemos que declarar los objetos para el contexto y parser, que junto con el objeto creado para los metadatos y el el \textit{FileInputStream} se pasarán a la función \textit{parse} como parámetros.\\

Finalmente guardaremos los links extraídos a partir del objeto \textit{LinkContentHandler} en una lista y para cada archivo los mostraremos en el caso de que no sea vacía.\\

\begin{lstlisting}
//Para cada archivo del directorio
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Definimos un handler para links
  LinkContentHandler linkHandler = new LinkContentHandler();
  //Definimos un objeto para el contexto y el parser
  ParseContext parseContext = new ParseContext();
  AutoDetectParser parser = new AutoDetectParser();
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, linkHandler, metadata, parseContext);
  } finally{
      stream.close();
  }
  //Guardamos los enlaces en una lista de Links
  List<Link> enlaces = linkHandler.getLinks();
  //Imprimimos aquellos enlaces que no sean vacios
  System.out.println("ENLACES del archivo " + file + ":\n");
  if(enlaces.isEmpty())
    System.out.println("NO se han encontrado enlaces.");
  else{
    for(Link l:enlaces)
      System.out.println(l);
}
\end{lstlisting}\

Podemos ver su funcionamiento en la Figura \ref{enlace}.

\clearpage

\begin{figure}[h]
\centering
\includegraphics[scale=0.33]{tabla.png}
\caption{Tabla creada con la opción \textbf{-d} para los archivos contenidos en \textit{docs}.}
\label{tabla}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[scale=0.32]{enlaces.png}
\caption{Extracto funcionamiento opción \textbf{-l} para 3 documentos.}
\label{enlace}
\end{figure}

\clearpage

\section{Opción \textbf{-t}.}

Para esta opción, contaremos las ocurrencias de cada palabra para cada archivo del directorio que recorramos. Además, generará un fichero CSV con el siguiente formato: $$palabra ; ocurrencias$$
Donde el número de ocurrencias será descendente.\\

Antes de empezar, crearemos (si no existe) una carpeta dentro del directorio \textit{src} llamada \textit{CSV}, donde generaremos los archivos CSV para cada fichero.\\

Definimos una función que introduce las entradas del Map en un stream, lo ordena con los valores del Map y crea otro Map con las entradas ordenadas y con los valores del antiguo.\\
  
\begin{lstlisting}
public static Map<String, Integer> sortByValue(final Map<String, Integer> wordCounts) {
  return wordCounts.entrySet().stream().sorted((Map.Entry.<String, Integer>comparingByValue().reversed())).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (e1, e2) -> e1, LinkedHashMap::new));
}
\end{lstlisting}\

Para cada archivo del directorio, realizaremos los siguientes pasos:

\begin{itemize}
\item Parseamos el archivo con el método \textit{parse} y pasamos el texto plano contenido en el \textit{handler} a un string.
\item Creamos un objeto stream a partir del string y mapeamos el elemento del stream a un array de strings con \textit{map()}. 
\item Los elementos del array de strings son las palabras separadas a través de \textit{split()}. Usaremos una expresión regular en el método \textit{split()} para separar las palabras por espacios y signos de puntuación.
\item Cada key tiene como valor 1 en el Map y las colisiones se resuelven sumando los valores. Así obtenemos un Map donde cada Key es la palabra que aparece y el valor su frecuencia.
\item Ordenamos el Map con la función definida \textit{sortByValue}.
\item Creamos un archivo \textit{.csv} para cada documento con las ocurrencias para cada palabra y lo guardamos en el directorio CSV.
\end{itemize}

\begin{lstlisting}
for(String file : archivos){
  File f = new File("./" + args[1] + "/" + file);
  //Establece el límite de caracteres en el constructor
  BodyContentHandler handler = new BodyContentHandler(1000000000);
  //Definimos un objeto para el contexto y el parser
  ParseContext parseContext = new ParseContext();
  AutoDetectParser parser = new AutoDetectParser();
  FileInputStream stream = new FileInputStream(f);
  try {
    parser.parse(stream, handler, metadata, parseContext);
  } finally{
      stream.close();}
  //Pasamos el contenido del documento del objeto handler a un string
  String str = handler.toString();
  //split() utiliza una expresion regular para separar las palabras.
  //flatMap() mapea el array a un stream que tiene como elementos los elementos del array. 
  //collect() nos devuelve una lista de strings con los elementos del stream
  List <String> list = Stream.of(str).map(w -> w.split("[\\s\\;\\.\\,\\:\\?\\¿\\¡\\!\\(\\)]+")).flatMap(Arrays::stream).collect(Collectors.toList());
  //stream() crea una Stream cuyos elementos son los elementos de list.
  //collect() nos devuelve un Map que se crea a traves de Collectors.toMap(). 
  //Collectors.toMap() crea un Map donde los keys son los elementos del stream. 
  Map <String, Integer > wordCounter = list.stream().collect(Collectors.toMap(w -> w.toLowerCase(), w -> 1, Integer::sum));
  //Ordenamos el Map segun los valores.
  wordCounter = sortByValue(wordCounter);
  Iterator it = wordCounter.keySet().iterator();
  //Creamos archivos .csv con el recuento de las palabras de cada documento en ./CSV
  PrintWriter writer = new PrintWriter("./CSV/" + file + ".csv");
  writer.print("Text" + ";" + "Size\n");
  while(it.hasNext()){
    String key = (String) it.next();
    writer.print(key + ";" + wordCounter.get(key) + "\n");
  }
  writer.close();
}
\end{lstlisting}

\clearpage

Para el documento \textit{quijote.txt} obtendremos un archivo \textit{.csv} que contiene todas las palabras del libro de Cervantes junto con su número de ocurrencias.\\

Como vimos en clase, las palabras más frecuentes son: \textit{que, de, y, la} como se puede apreciar en el fragmento del archivo obtenido.\\

\begin{lstlisting}
Text;Size
que;20413
y;17962
de;17947
la;10224
...
quijote;2158
sancho;2140
es;2118
\end{lstlisting}\

Haciendo una representación gráfica en escala logarítmica del ranking de palabras frente al número de ocurrencias, podemos ver cómo el Quijote, cumple la \textbf{ley de Zipf}: la posición en el ranking de una palabra ($R$) por su frecuencia ($F$) es aprox. constante ($k$). \cite{ziki}$$F=\frac{k}{R}$$

\begin{figure}[h]
\centering
\includegraphics[scale=0.74]{Zipf.png}
\caption{Gráfica en escala logarítmica para el archivo \textit{quijote.txt.csv}}.
\label{zipf}
\end{figure}


\section{Nube de palabras.}

Podemos generar nubes de palabras a partir de los ficheros CSV obtenidos con la opción \textbf{-t} desde la web de \textit{wordart}. \cite{Nube}\\

Hemos elegido dos de entre todos los ficheros CSV generados y a continuación mostraremos el resultado. Además, hemos configurado una forma y color diferente para cada nube generada.\\

Podemos ver las nubes de palabras resultantes en la Figura \ref{nubes}.

\section{Método de compilación.}

Ejecutamos las siguientes órdenes a través de terminal situados en el directorio \textit{src}.

\begin{lstlisting}
> javac -cp ./tika-app-1.23.jar P1Tika.java
> java -cp ./tika-app-1.23.jar:. P1Tika -option docs
\end{lstlisting}

Donde \textit{-option} puede ser una de las opciones de la siguiente lista $[-d,-l,-t]$.

\section{Trabajo en Grupo.}

El trabajo lo hemos repartido, en primera instancia, de la siguiente manera:

\begin{itemize}
\item \textbf{Daniel Bolaños Martínez}: Opción -d (formato de tabla),  Opción -l y elaboración de la memoria.
\item \textbf{Fernando de la Hoz Moreno}: Opción -d y Opción -t.
\end{itemize}

No obstante, hemos mantenido el contacto durante el desarrollo de la práctica y hemos colaborado conjuntamente en la elaboración del proyecto en su totalidad.

\clearpage

\begin{figure}[h]
\centering
\includegraphics[scale=0.67]{nube1.png}
\includegraphics[scale=0.55]{blanc.png}
\includegraphics[scale=0.67]{nube2.png}
\caption{Nubes para \textit{Resumen\_Daniel\_Fernando.pdf.csv} y \textit{quijote.txt.csv}}
\label{nubes}
\end{figure}

\clearpage

\patchcmd{\thebibliography}{\section*}{\section}{}{}
\begin{thebibliography}{}
\bibitem{Guion} Guión de la práctica 1 de la asignatura.
\bibitem{tika} \url{https://tika.apache.org/1.24.1/examples.html}
\bibitem{Nube} \url{https://wordart.com/create}
\bibitem{ziki} \url{https://en.wikipedia.org/wiki/Zipf%27s_law}
\end{thebibliography}

\end{document}